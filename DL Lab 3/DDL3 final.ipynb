{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1 align = \"center\">Experiment 3</h1>\n",
        "\n",
        "[Aim: To implement Stochastic Gradient Descent](https://realpython.com/gradient-descent-algorithm-python/)"
      ],
      "metadata": {
        "id": "zcTxB5qR7QHQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ycxoSYRArGsI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def sgd(\n",
        "    gradient, x, y, start, learn_rate=0.1, batch_size=1, n_iter=50,\n",
        "    tolerance=1e-06, dtype=\"float64\", random_state=None\n",
        "):\n",
        "    # Checking if the gradient is callable\n",
        "    if not callable(gradient):\n",
        "        raise TypeError(\"'gradient' must be callable\")\n",
        "    # Setting up the data type for NumPy arrays\n",
        "    dtype_ = np.dtype(dtype)\n",
        "    # Converting x and y to NumPy arrays\n",
        "    x, y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)\n",
        "    n_obs = x.shape[0]\n",
        "    if n_obs != y.shape[0]:\n",
        "        raise ValueError(\"'x' and 'y' lengths do not match\")\n",
        "    xy = np.c_[x.reshape(n_obs, -1), y.reshape(n_obs, 1)]\n",
        "    # Initializing the random number generator\n",
        "    seed = None if random_state is None else int(random_state)\n",
        "    rng = np.random.default_rng(seed=seed)\n",
        "    # Initializing the values of the variables\n",
        "    vector = np.array(start, dtype=dtype_)\n",
        "    # Setting up and checking the learning rate\n",
        "    learn_rate = np.array(learn_rate, dtype=dtype_)\n",
        "    if np.any(learn_rate <= 0):\n",
        "        raise ValueError(\"'learn_rate' must be greater than zero\")\n",
        "    # Setting up and checking the size of minibatches\n",
        "    batch_size = int(batch_size)\n",
        "    if not 0 < batch_size <= n_obs:\n",
        "        raise ValueError(\n",
        "            \"'batch_size' must be greater than zero and less than \"\n",
        "            \"or equal to the number of observations\"\n",
        "        )\n",
        "    # Setting up and checking the maximal number of iterations\n",
        "    n_iter = int(n_iter)\n",
        "    if n_iter <= 0:\n",
        "        raise ValueError(\"'n_iter' must be greater than zero\")\n",
        "    # Setting up and checking the tolerance\n",
        "    tolerance = np.array(tolerance, dtype=dtype_)\n",
        "    if np.any(tolerance <= 0):\n",
        "        raise ValueError(\"'tolerance' must be greater than zero\")\n",
        "    # Performing the gradient descent loop\n",
        "    for _ in range(n_iter):\n",
        "        # Shuffle x and y\n",
        "        rng.shuffle(xy)\n",
        "        # Performing minibatch moves\n",
        "        for start in range(0, n_obs, batch_size):\n",
        "            stop = start + batch_size\n",
        "            x_batch, y_batch = xy[start:stop, :-1], xy[start:stop, -1:]\n",
        "            # Recalculating the difference\n",
        "            grad = np.array(gradient(x_batch, y_batch, vector), dtype_)\n",
        "            print(\"gradient \", grad)\n",
        "            diff = -learn_rate * grad\n",
        "\n",
        "            # Checking if the absolute difference is small enough\n",
        "            if np.all(np.abs(diff) <= tolerance):\n",
        "                break\n",
        "\n",
        "            # Updating the values of the variables\n",
        "            vector += diff\n",
        "        print(\"epoch no : \",_, \"  \",vector ,\"  \",diff)\n",
        "    return vector if vector.shape else vector.item()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([5, 15, 25, 35, 45, 55])\n",
        "y = np.array([5, 20, 14, 32, 22, 38])"
      ],
      "metadata": {
        "id": "88zIlRkV7ZjD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ssr_gradient(x, y, b):\n",
        "    res = b[0] + b[1] * x - y\n",
        "    return res.mean(), (res * x).mean()"
      ],
      "metadata": {
        "id": "-cKCAwAZ7pIW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def gradient_descent(\n",
        "    gradient, x, y, start, learn_rate=0.1, n_iter=50, tolerance=1e-06\n",
        "):\n",
        "    vector = start\n",
        "    for _ in range(n_iter):\n",
        "        diff = -learn_rate * np.array(gradient(x, y, vector))\n",
        "        if np.all(np.abs(diff) <= tolerance):\n",
        "            break\n",
        "        vector += diff\n",
        "        print(\"epoch no : \",_, \"  \",vector ,\" \",diff)\n",
        "    return vector"
      ],
      "metadata": {
        "id": "28vosjJa_JCQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_descent(\n",
        "     ssr_gradient, x, y, start=[0.5, 0.5], learn_rate=0.0008,\n",
        "      n_iter=10\n",
        ")"
      ],
      "metadata": {
        "id": "4n7g4r0z_KMg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a69ed21-33a2-4029-a844-f24bafd6dfa3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch no :  0    [0.50506667 0.66133333]   [0.00506667 0.16133333]\n",
            "epoch no :  1    [0.50625728 0.66874062]   [0.00119061 0.00740729]\n",
            "epoch no :  2    [0.50726917 0.66905772]   [0.00101189 0.0003171 ]\n",
            "epoch no :  3    [0.50827263 0.66904823]   [ 1.00346603e-03 -9.48731962e-06]\n",
            "epoch no :  4    [0.50927552 0.66902371]   [ 1.00289095e-03 -2.45259263e-05]\n",
            "epoch no :  5    [0.5102782  0.66899849]   [ 1.00267726e-03 -2.52139261e-05]\n",
            "epoch no :  6    [0.51128068 0.66897325]   [ 1.00248026e-03 -2.52409042e-05]\n",
            "epoch no :  7    [0.51228296 0.66894802]   [ 1.00228405e-03 -2.52374350e-05]\n",
            "epoch no :  8    [0.51328505 0.66892278]   [ 1.00208792e-03 -2.52325642e-05]\n",
            "epoch no :  9    [0.51428694 0.66889756]   [ 1.00189183e-03 -2.52276298e-05]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.51428694, 0.66889756])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sgd(\n",
        "     ssr_gradient, x, y, start=[0.5, 0.5], learn_rate=0.0008,\n",
        "     batch_size=3, n_iter=10, random_state=0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVH4-qAK7XVO",
        "outputId": "f094351e-add1-4281-f826-df63b4c3f0e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gradient  [  -8.33333333 -355.        ]\n",
            "gradient  [  1.82666667 167.17777778]\n",
            "epoch no :  0    [0.50520533 0.65025778]    [-0.00146133 -0.13374222]\n",
            "gradient  [ -2.740928   -27.76340148]\n",
            "gradient  [ 0.10181386 29.41285707]\n",
            "epoch no :  1    [0.50731662 0.64893821]    [-8.14510899e-05 -2.35302857e-02]\n",
            "gradient  [ -0.10610067 -12.34338434]\n",
            "gradient  [ -3.296856   -24.14065244]\n",
            "epoch no :  2    [0.51003899 0.67812544]    [0.00263748 0.01931252]\n",
            "gradient  [  -6.05766123 -138.90427584]\n",
            "gradient  [  8.73358776 371.11597989]\n",
            "epoch no :  3    [0.50789825 0.49235608]    [-0.00698687 -0.29689278]\n",
            "gradient  [  -3.64393177 -190.7973226 ]\n",
            "gradient  [  -6.66429775 -155.17973513]\n",
            "epoch no :  4    [0.51614483 0.76913773]    [0.00533144 0.12414379]\n",
            "gradient  [  3.64171373 205.04775736]\n",
            "gradient  [  -5.32528375 -199.81418802]\n",
            "epoch no :  5    [0.51749169 0.76495087]    [0.00426023 0.15985135]\n",
            "gradient  [ 1.84060838 76.21567881]\n",
            "gradient  [1.02216290e-01 1.06697314e+02]\n",
            "epoch no :  6    [0.51593743 0.61862048]    [-8.17730321e-05 -8.53578513e-02]\n",
            "gradient  [  -5.01855067 -116.77421148]\n",
            "gradient  [  2.77468018 117.91646259]\n",
            "epoch no :  7    [0.51773253 0.61770667]    [-0.00221974 -0.09433317]\n",
            "gradient  [-1.76528952 41.31176656]\n",
            "gradient  [  -5.06899354 -216.69806737]\n",
            "epoch no :  8    [0.52319995 0.75801572]    [0.00405519 0.17335845]\n",
            "gradient  [  5.80692618 233.97734521]\n",
            "gradient  [  -9.50226121 -300.35678512]\n",
            "epoch no :  9    [0.52615622 0.81111927]    [0.00760181 0.24028543]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.52615622, 0.81111927])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}